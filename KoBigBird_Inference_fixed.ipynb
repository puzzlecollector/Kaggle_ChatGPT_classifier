{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1901831-a8e5-463e-8e59-f34250ba8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "from transformers import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "import time\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c82b21-e927-46a9-9ed2-f17a9ed2c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\") \n",
    "\n",
    "class MeanPooling(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(MeanPooling, self).__init__() \n",
    "    def forward(self, last_hidden_state, attention_masks): \n",
    "        input_mask_expanded = attention_masks.unsqueeze(-1).expand(last_hidden_state.size()).float() \n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) \n",
    "        sum_mask = input_mask_expanded.sum(1) \n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9) \n",
    "        mean_embeddings = sum_embeddings / sum_mask \n",
    "        return mean_embeddings \n",
    "    \n",
    "class MultiSampleDropout(nn.Module): \n",
    "    def __init__(self, max_dropout_rate, num_samples, classifier):\n",
    "        super(MultiSampleDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout\n",
    "        self.classifier = classifier\n",
    "        self.max_dropout_rate = max_dropout_rate\n",
    "        self.num_samples = num_samples \n",
    "    def forward(self, out):\n",
    "        return torch.mean(torch.stack([self.classifier(self.dropout(p=rate)(out)) for _, rate in enumerate(np.linspace(0, self.max_dropout_rate, self.num_samples))], dim=0), dim=0)\n",
    "\n",
    "class ChatGPTDetector(nn.Module): \n",
    "    def __init__(self, num_classes=2): \n",
    "        super(ChatGPTDetector, self).__init__() \n",
    "        self.num_classes = num_classes \n",
    "        self.config = AutoConfig.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "        self.lm = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\") \n",
    "        self.mean_pooler = MeanPooling() \n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.num_classes) \n",
    "        self._init_weights(self.fc) \n",
    "        self.multi_dropout = MultiSampleDropout(0.2, 8, self.fc) \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear): \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) \n",
    "            if module.bias is not None: \n",
    "                module.bias.data.zero_() \n",
    "    def forward(self, input_ids, attn_masks):\n",
    "        x = self.lm(input_ids, attn_masks)[0]\n",
    "        x = self.mean_pooler(x, attn_masks) \n",
    "        x = self.multi_dropout(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db984c7b-c34a-4921-bdea-c8e95f9796df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGPTDetector(\n",
       "  (lm): BigBirdModel(\n",
       "    (embeddings): BigBirdEmbeddings(\n",
       "      (word_embeddings): Embedding(32500, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(4096, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BigBirdEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BigBirdLayer(\n",
       "          (attention): BigBirdAttention(\n",
       "            (self): BigBirdBlockSparseAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BigBirdSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BigBirdIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): NewGELUActivation()\n",
       "          )\n",
       "          (output): BigBirdOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (mean_pooler): MeanPooling()\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (multi_dropout): MultiSampleDropout(\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGPTDetector()\n",
    "checkpoint = torch.load(\"KR_ChatGPT_Detector_v1_.pt\")\n",
    "model.load_state_dict(checkpoint) \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee9c16d-fbf5-4f9f-a7ab-2c96950e5917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT가 생성했을 확률: 99.98743438720703%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6773/3078326761.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_class = nn.Softmax()(output)[0]\n"
     ]
    }
   ],
   "source": [
    "# 실제 ChatGPT가 생성한 텍스트 \n",
    "passage = \"\"\"스마트 컨트랙트는 블록체인 기술에서 사용되는 프로그램 코드의 일종으로, 계약 조건을 자동으로 검증하고 실행하는 프로그램입니다. 스마트 컨트랙트는 블록체인 네트워크 상에서 실행되며, 블록체인에서 발생하는 모든 트랜잭션은 스마트 컨트랙트를 통해 처리됩니다.\n",
    "\n",
    "스마트 컨트랙트는 조건과 실행 코드로 이루어져 있습니다. 예를 들어, A가 B에게 1,000달러를 지불해야하는 계약이 있다면, 이 계약의 조건을 스마트 컨트랙트로 작성하여 계약이 자동으로 실행되도록 할 수 있습니다. 이러한 스마트 컨트랙트는 자동으로 조건을 검증하고, 지정된 조건이 충족되었을 때 계약의 실행 코드를 실행하여 계약을 이행합니다. 이를 통해 계약 당사자는 서로를 신뢰하지 않아도 안전하게 거래를 진행할 수 있습니다.\n",
    "\n",
    "스마트 컨트랙트는 블록체인에서 실행되기 때문에 모든 거래 내역이 투명하게 기록되며, 중개인이나 중앙 기관의 개입이 없기 때문에 거래 비용이 줄어듭니다. 또한 스마트 컨트랙트는 코드로 작성되기 때문에 자동화가 가능하며, 프로그램에 따른 비즈니스 로직을 실행하는 것이 가능합니다. 따라서 스마트 컨트랙트는 블록체인 기술의 핵심 기능 중 하나로, 분산형 애플리케이션(DApp) 개발 등에 활용되고 있습니다.\"\"\"\n",
    "\n",
    "\n",
    "encoded_input = tokenizer(passage, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device) \n",
    "input_ids = encoded_input[\"input_ids\"] \n",
    "attn_masks = encoded_input[\"attention_mask\"] \n",
    "\n",
    "with torch.no_grad(): \n",
    "    output = model(input_ids, attn_masks) \n",
    "    pred_class = nn.Softmax()(output)[0]\n",
    "    print(f\"ChatGPT가 생성했을 확률: {pred_class[1]*100}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ba38-7c0a-4d2d-b903-7dfc77ab0c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477b702e-2aab-47e2-8ad5-7838c887285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--monologg--kobigbird-bert-base/snapshots/ceacda477e20abef2c929adfa4a07c6f811323be/pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bigbird = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "bigbird_state_dict = bigbird.state_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e640f5-a564-4358-bcf2-4f77da522fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0 \n",
    "for key, value in bigbird_state_dict.items(): \n",
    "    check_key = \"lm.\" + str(key) \n",
    "    if check_key in checkpoint.keys():\n",
    "        bigbird_state_dict[key] = checkpoint[check_key] \n",
    "        cnt += 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0088c02-71b7-4b19-8e82-87157a4f9f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb768ce-e29d-4b28-ad2b-13a60c4a98eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigbird.load_state_dict(bigbird_state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b60c22-2174-4420-9503-85418e655e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bigbird.state_dict(), \"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84a8a3a-abc1-4c43-8057-48edd7ddcb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"axiomlabs/Ko_Wiki_ChatGPT_Detector_v1\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BigBirdForSequenceClassification.\n",
      "\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at axiomlabs/Ko_Wiki_ChatGPT_Detector_v1 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--axiomlabs--Ko_Wiki_ChatGPT_Detector_v1/snapshots/2739517250a89499a199beb3e312057e83247ed3/tokenizer_config.json\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110/2062807357.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probability = nn.Softmax()(outputs)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"axiomlabs/Ko_Wiki_ChatGPT_Detector_v1\", num_labels=2) \n",
    "model.eval() \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"axiomlabs/Ko_Wiki_ChatGPT_Detector_v1\")\n",
    "\n",
    "text = '''스마트 컨트랙트는 블록체인 기술에서 사용되는 프로그램 코드의 일종으로, 계약 조건을 자동으로 검증하고 실행하는 프로그램입니다. 스마트 컨트랙트는 블록체인 네트워크 상에서 실행되며, 블록체인에서 발생하는 모든 트랜잭션은 스마트 컨트랙트를 통해 처리됩니다.\n",
    "\n",
    "스마트 컨트랙트는 조건과 실행 코드로 이루어져 있습니다. 예를 들어, A가 B에게 1,000달러를 지불해야하는 계약이 있다면, 이 계약의 조건을 스마트 컨트랙트로 작성하여 계약이 자동으로 실행되도록 할 수 있습니다. 이러한 스마트 컨트랙트는 자동으로 조건을 검증하고, 지정된 조건이 충족되었을 때 계약의 실행 코드를 실행하여 계약을 이행합니다. 이를 통해 계약 당사자는 서로를 신뢰하지 않아도 안전하게 거래를 진행할 수 있습니다.\n",
    "\n",
    "스마트 컨트랙트는 블록체인에서 실행되기 때문에 모든 거래 내역이 투명하게 기록되며, 중개인이나 중앙 기관의 개입이 없기 때문에 거래 비용이 줄어듭니다. 또한 스마트 컨트랙트는 코드로 작성되기 때문에 자동화가 가능하며, 프로그램에 따른 비즈니스 로직을 실행하는 것이 가능합니다. 따라서 스마트 컨트랙트는 블록체인 기술의 핵심 기능 중 하나로, 분산형 애플리케이션(DApp) 개발 등에 활용되고 있습니다.'''\n",
    "\n",
    "encoded_inputs = tokenizer(text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "with torch.no_grad(): \n",
    "    outputs = model(**encoded_inputs).logits \n",
    "    probability = nn.Softmax()(outputs) \n",
    "    predicted_class = torch.argmax(probability).item() \n",
    "    if predicted_class == 1: \n",
    "        print(\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24041b1f-c1a6-4f60-8221-6ac8bdca8e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
